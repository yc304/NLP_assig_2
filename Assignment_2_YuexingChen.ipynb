{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#a.[a-zA-Z]+\n",
    "#b.[A-Z][a-z]*\n",
    "#c.p[aeiou]{,2}t\n",
    "#d.\\d+(\\.\\d+)?\n",
    "#e.([^aeiou][aeiou][^aeiou])*\n",
    "#f.\\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a: has one or more alphabet regardless of upper or lower case.\n",
    "# b: Start with upper case and followed by lower case,\n",
    "   # where lower cases can be omitted.\n",
    "# c: start with ‘p’ and end with ‘t’, between them 0 to 2 vowels \n",
    "   # like 'a,e,i,o,u' can be inserted.\n",
    "# d: ‘\\d’ represents numbers, it means numbers with decimals\n",
    "   # and in which under decimal point is optional.\n",
    "# e: lines which no vowels at the beginning or end, and which\n",
    "   # have some vowels in between\n",
    "# f: alphanumeric characters or sequence without whitspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Rewrite the following loop as a list comprehension:\n",
    "# sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "# result = []\n",
    "# for word in sent:\n",
    "#     word_len = (word, len(word))\n",
    "#     result.append(word_len)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['This', 'is', 'an', 'introduction', 'class']   # Rewrite as list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, len(word)) for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Read in some text from your own document in your local disk, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('hw2.txt')   # Here I choose the document.txt file mentioned in class which includes wh-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What book are you reading?\\nWhich plane is he catching?\\nWhose jacket is this?\\nor about the pronoun one or ones.\\nWhich one would you like?\\nWhich ones did Ruth want?\\nThe determiner which can be used in questions about selecting.\\nIt can also be used together with the preposition of for the same purpose.\\nWhich colour shall we use?\\nWhich book sells the most copies?\\nWhich of these colours shall we use?\\nOf all your novels,which of them did you enjoy writing the most?\\nThe determiner whose asks about possession with reference to a person as the possessor.\\nWhose mother did you say she was?\\nWhose bag is this?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)   # tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = [word for word in tokens if word.startswith('wh') or word.startswith('Wh')]   # pull out all wh-words and print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'Which', 'Which', 'Which', 'Which', 'Which', 'Which', 'Whose', 'Whose', 'Whose', 'which', 'which', 'whose']\n"
     ]
    }
   ],
   "source": [
    "wh_words.sort()\n",
    "print(wh_words)   # sort the list and print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('hw2_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [line.strip() for line in f]   # reads in line & strip out new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [line.split(' ') for line in f]   # split lines into component pieces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result:   # convert num to integer\n",
    "    item[1] = int(item[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "print(result[0][1]+1)   # Use first \"This 120\" to test num is num: 53+1=54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,… Humor) (Hint: for category in brown.categories( )). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences. (Hint: from nltk.corpus import brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_num_words_per_sentence(text, category):   # avg num of words per sentence =  num of words / number of sentences\n",
    "    num_sent = len(text.sents(categories=category))\n",
    "    num_word = len(text.words(categories=category))\n",
    "    return num_word / num_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_num_letters(text, category):   # avg num of letters per word\n",
    "    num_word = len(text.words(categories=category))\n",
    "    smash_text = ''.join(text.words(categories=category))\n",
    "    len_letter = len(smash_text)\n",
    "    return len_letter / num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ari(text, category):   # ARI = 4.71μw + 0.5μs - 21.43\n",
    "    uw = average_num_letters(text, category)\n",
    "    us = average_num_words_per_sentence(text, category)\n",
    "    ari = (4.71*uw) + (0.5*us) - 21.43\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure: 4.0841684990890705\n",
      "belles_lettres: 10.987652885621749\n",
      "editorial: 9.471025332953673\n",
      "fiction: 4.9104735321302115\n",
      "government: 12.08430349501021\n",
      "hobbies: 8.922356393630267\n",
      "humor: 7.887805248319808\n",
      "learned: 11.926007043317348\n",
      "lore: 10.254756197101155\n",
      "mystery: 3.8335518942055167\n",
      "news: 10.176684595052684\n",
      "religion: 10.203109907301261\n",
      "reviews: 10.769699888473433\n",
      "romance: 4.34922419804213\n",
      "science_fiction: 4.978058336905399\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    print(category + ': ' + str(ari(brown, category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers.\n",
    "# text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'base',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'are',\n",
       " 'becom',\n",
       " 'increasingli',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructur',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'translat',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysi',\n",
       " 'enabl',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'By',\n",
       " 'provid',\n",
       " 'more',\n",
       " 'natur',\n",
       " 'human-machin',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophist',\n",
       " 'access',\n",
       " 'to',\n",
       " 'store',\n",
       " 'inform',\n",
       " ',',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingu',\n",
       " 'inform',\n",
       " 'societi',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'bas',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'ar',\n",
       " 'becom',\n",
       " 'increas',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phon',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'giv',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstruct',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'transl',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'writ',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'span',\n",
       " ';',\n",
       " 'text',\n",
       " 'analys',\n",
       " 'en',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'senty',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'by',\n",
       " 'provid',\n",
       " 'mor',\n",
       " 'nat',\n",
       " 'human-machine',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'mor',\n",
       " 'soph',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stor',\n",
       " 'inform',\n",
       " ',',\n",
       " 'langu',\n",
       " 'process',\n",
       " 'has',\n",
       " 'com',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'cent',\n",
       " 'rol',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multil',\n",
       " 'inform',\n",
       " 'socy',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "# Main difference: Two algorithms are different, reflected in the output\n",
    "                 # for single word.\n",
    "# Porter Stemmer: Words are in the original alphabetical order, sometimes\n",
    "                # the following letters are omitted.\n",
    "# Lancaster Stemmer: Words are not necessarily in the original alphabetical\n",
    "                   # order (sentiment v.s. senty), sometimes a few letters\n",
    "                   # in the word are extracted which makes the output more\n",
    "                   # abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. Please compare the reading difficulties for ABC Rural News (\"rural.txt\") and ABC Science News(\"science.txt\") (nltk.corpus.abc).(Hint: from nltk.corpus import abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = len(abc.words('rural.txt'))\n",
    "num_sent = len(abc.sents('rural.txt'))\n",
    "smash_text = ''.join((i for i in abc.words('rural.txt') if i.isalpha()))\n",
    "len_letter = len(smash_text)\n",
    "\n",
    "uw = len_letter / num_word\n",
    "us = num_word / num_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading difficulity of rural.txt: 11.66583650667559\n"
     ]
    }
   ],
   "source": [
    "print('Reading difficulity of rural.txt:', 4.71*uw+0.5*us-21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = len(abc.words('science.txt'))\n",
    "num_sent = len(abc.sents('science.txt'))\n",
    "smash_text = ''.join((i for i in abc.words('science.txt') if i.isalnum()))\n",
    "len_letter = len(smash_text)\n",
    "\n",
    "uw = len_letter / num_word\n",
    "us = num_word / num_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading difficulity of science.txt: 11.912359168733985\n"
     ]
    }
   ],
   "source": [
    "print('Reading difficulity of science.txt:', 4.71*uw+0.5*us-21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Rewrite the following nested loop as a nested list comprehension:\n",
    "#  words = ['attribution', 'confabulation', 'elocution',\n",
    "#         'sequoia', 'tenacious', 'unidirectional']\n",
    "#  vsequences = set()\n",
    "#  for word in words:\n",
    "#      vowels = []\n",
    "#      for char in word:\n",
    "#          if char in 'aeiou':\n",
    "#              vowels.append(char)\n",
    "#      vsequences.add(''.join(vowels))\n",
    "#  sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [''.join(L) for L in map(lambda x:[i for i in x if i in 'aeiou' ], words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "# output should look like:\n",
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816\n",
    "# sample code:\n",
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
      "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
      "Emma                   was written by Jane Austen         in 1816\n"
     ]
    }
   ],
   "source": [
    "book = ['The Tragedie of Hamlet','Leaves of Grass','Emma']\n",
    "name = ['William Shakespeare','Walt Whiteman','Jane Austen']\n",
    "year = [1599,1855,1816]\n",
    "template = '{:22} was written by {:19} in {}'\n",
    "for i in range(3): \n",
    "    print(template.format(book[i],name[i],year[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Define the variable quote to contain the list ['Action', 'speaks', 'louder', 'than', 'words']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Then do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['Action', 'speaks', 'louder', 'than', 'words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Action', 6), ('speaks', 6), ('louder', 6), ('than', 4), ('words', 5)]\n"
     ]
    }
   ],
   "source": [
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    length.append(word_len)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Action', 6), ('speaks', 6), ('louder', 6), ('than', 4), ('words', 5)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word,len(word)) for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
